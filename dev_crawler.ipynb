{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 12:17:33 WARN Utils: Your hostname, DESKTOP-L7KNHUE resolves to a loopback address: 127.0.1.1; using 192.168.238.57 instead (on interface eth0)\n",
      "23/04/10 12:17:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 12:17:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for crawling\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "## Creates spark session\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('Dota2 Crawler').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe with schema for the dataset\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "## Create schema for in loops\n",
    "schema = StructType([\n",
    "  StructField('type', StringType(), True),\n",
    "  StructField('counter', StringType(), True),\n",
    "  StructField('srcline', IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "## Create schema for main dataframe\n",
    "main_schema = StructType([\n",
    "  StructField('hero', StringType(), True),\n",
    "  StructField('segment', StringType(), True),\n",
    "  StructField('type', StringType(), True),\n",
    "  StructField('counter', StringType(), True),\n",
    "  StructField('srcline', IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(emptyRDD,schema=main_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all heroes\n",
    "page_hero = requests.get('https://dota2.fandom.com/wiki/Category:Heroes')\n",
    "soup_hero = BeautifulSoup(page_hero.text, 'html.parser')\n",
    "heroes = []\n",
    "for i in soup_hero.find(class_=\"mw-content-ltr\").find_all(\"a\"):\n",
    "    if '/Category:' not in i['href']:\n",
    "        heroes.append(i.get_text())\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abaddon\n",
      "Abaddondone\n",
      "Alchemist\n",
      "Alchemistdone\n",
      "Ancient Apparition\n",
      "Ancient Apparitiondone\n",
      "Anti-Mage\n",
      "Anti-Magedone\n",
      "Arc Warden\n",
      "Arc Wardendone\n",
      "Axe\n",
      "Axedone\n",
      "Bane\n",
      "Banedone\n",
      "Batrider\n",
      "Batriderdone\n",
      "Beastmaster\n",
      "Beastmasterdone\n",
      "Bloodseeker\n",
      "Bloodseekerdone\n",
      "Bounty Hunter\n",
      "Bounty Hunterdone\n",
      "Brewmaster\n",
      "Brewmasterdone\n",
      "Bristleback\n",
      "Bristlebackdone\n",
      "Broodmother\n",
      "Broodmotherdone\n",
      "Centaur Warrunner\n",
      "Centaur Warrunnerdone\n",
      "Chaos Knight\n",
      "Chaos Knightdone\n",
      "Chen\n",
      "Chendone\n",
      "Clinkz\n",
      "Clinkzdone\n",
      "Clockwerk\n",
      "Clockwerkdone\n",
      "Crystal Maiden\n",
      "Crystal Maidendone\n",
      "Dark Seer\n",
      "Dark Seerdone\n",
      "Dark Willow\n",
      "Dark Willowdone\n",
      "Dawnbreaker\n",
      "Dawnbreakerdone\n",
      "Dazzle\n",
      "Dazzledone\n",
      "Death Prophet\n",
      "Death Prophetdone\n",
      "Disruptor\n",
      "Disruptordone\n",
      "Doom\n",
      "Doomdone\n",
      "Dragon Knight\n",
      "Dragon Knightdone\n",
      "Drow Ranger\n",
      "Drow Rangerdone\n",
      "Earth Spirit\n",
      "Earth Spiritdone\n",
      "Earthshaker\n",
      "Earthshakerdone\n",
      "Elder Titan\n",
      "Elder Titandone\n",
      "Ember Spirit\n",
      "Ember Spiritdone\n",
      "Enchantress\n",
      "Enchantressdone\n",
      "Enigma\n",
      "Enigmadone\n",
      "Faceless Void\n",
      "Faceless Voiddone\n",
      "Grimstroke\n",
      "Grimstrokedone\n",
      "Gyrocopter\n",
      "Gyrocopterdone\n",
      "Hoodwink\n",
      "Hoodwinkdone\n",
      "Huskar\n",
      "Huskardone\n",
      "Invoker\n",
      "Invokerdone\n",
      "Io\n",
      "Iodone\n",
      "Jakiro\n",
      "Jakirodone\n",
      "Juggernaut\n",
      "Juggernautdone\n",
      "Keeper of the Light\n",
      "Keeper of the Lightdone\n",
      "Kunkka\n",
      "Kunkkadone\n",
      "Legion Commander\n",
      "Legion Commanderdone\n",
      "Leshrac\n",
      "Leshracdone\n",
      "Lich\n",
      "Lichdone\n",
      "Lifestealer\n",
      "Lifestealerdone\n",
      "Lina\n",
      "Linadone\n",
      "Lion\n",
      "Liondone\n",
      "Lone Druid\n",
      "Lone Druiddone\n",
      "Luna\n",
      "Lunadone\n",
      "Lycan\n",
      "Lycandone\n",
      "Magnus\n",
      "Magnusdone\n",
      "Marci\n",
      "Marcidone\n",
      "Mars\n",
      "Marsdone\n",
      "Medusa\n",
      "Medusadone\n",
      "Meepo\n",
      "Meepodone\n",
      "Mirana\n",
      "Miranadone\n",
      "Monkey King\n",
      "Monkey Kingdone\n",
      "Morphling\n",
      "Morphlingdone\n",
      "Muerta\n",
      "Muertadone\n",
      "Naga Siren\n",
      "Naga Sirendone\n",
      "Nature's Prophet\n",
      "Nature's Prophetdone\n",
      "Necrophos\n",
      "Necrophosdone\n",
      "Night Stalker\n",
      "Night Stalkerdone\n",
      "Nyx Assassin\n",
      "Nyx Assassindone\n",
      "Ogre Magi\n",
      "Ogre Magidone\n",
      "Omniknight\n",
      "Omniknightdone\n",
      "Oracle\n",
      "Oracledone\n",
      "Outworld Destroyer\n",
      "Outworld Destroyerdone\n",
      "Pangolier\n",
      "Pangolierdone\n",
      "Phantom Assassin\n",
      "Phantom Assassindone\n",
      "Phantom Lancer\n",
      "Phantom Lancerdone\n",
      "Phoenix\n",
      "Phoenixdone\n",
      "Primal Beast\n",
      "Primal Beastdone\n",
      "Puck\n",
      "Puckdone\n",
      "Pudge\n",
      "Pudgedone\n",
      "Pugna\n",
      "Pugnadone\n",
      "Queen of Pain\n",
      "Queen of Paindone\n",
      "Razor\n",
      "Razordone\n",
      "Riki\n",
      "Rikidone\n",
      "Rubick\n",
      "Rubickdone\n",
      "Sand King\n",
      "Sand Kingdone\n",
      "Shadow Demon\n",
      "Shadow Demondone\n",
      "Shadow Fiend\n",
      "Shadow Fienddone\n",
      "Shadow Shaman\n",
      "Shadow Shamandone\n",
      "Silencer\n",
      "Silencerdone\n",
      "Skywrath Mage\n",
      "Skywrath Magedone\n",
      "Slardar\n",
      "Slardardone\n",
      "Slark\n",
      "Slarkdone\n",
      "Snapfire\n",
      "Snapfiredone\n",
      "Sniper\n",
      "Sniperdone\n",
      "Spectre\n",
      "Spectredone\n",
      "Spirit Breaker\n",
      "Spirit Breakerdone\n",
      "Storm Spirit\n",
      "Storm Spiritdone\n",
      "Sven\n",
      "Svendone\n",
      "Techies\n",
      "Techiesdone\n",
      "Templar Assassin\n",
      "Templar Assassindone\n",
      "Terrorblade\n",
      "Terrorbladedone\n",
      "Tidehunter\n",
      "Tidehunterdone\n",
      "Timbersaw\n",
      "Timbersawdone\n",
      "Tinker\n",
      "Tinkerdone\n",
      "Tiny\n",
      "Tinydone\n",
      "Treant Protector\n",
      "Treant Protectordone\n",
      "Troll Warlord\n",
      "Troll Warlorddone\n",
      "Tusk\n",
      "Tuskdone\n",
      "Underlord\n",
      "Underlorddone\n",
      "Undying\n",
      "Undyingdone\n",
      "Ursa\n",
      "Ursadone\n",
      "Vengeful Spirit\n",
      "Vengeful Spiritdone\n",
      "Venomancer\n",
      "Venomancerdone\n",
      "Viper\n",
      "Viperdone\n",
      "Visage\n",
      "Visagedone\n",
      "Void Spirit\n",
      "Void Spiritdone\n",
      "Warlock\n",
      "Warlockdone\n",
      "Weaver\n",
      "Weaverdone\n",
      "Windranger\n",
      "Windrangerdone\n",
      "Winter Wyvern\n",
      "Winter Wyverndone\n",
      "Witch Doctor\n",
      "Witch Doctordone\n",
      "Wraith King\n",
      "Wraith Kingdone\n",
      "Zeus\n",
      "Zeusdone\n"
     ]
    }
   ],
   "source": [
    "for hero in heroes:\n",
    "    print('Scraping hero page: '+hero)\n",
    "    # initiate beautiful soup scraping on a page\n",
    "    page = requests.get(f'https://dota2.fandom.com/wiki/{hero}/Counters')\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Pull all text from body div\n",
    "    hero_name_list = soup.find_all(style='font-size:12pt;display:inline;')\n",
    "    header_list = soup.find_all(class_='mw-headline')\n",
    "    item_list = soup.find_all(class_='image-link')\n",
    "\n",
    "    # function that outputs a range to categorize the type\n",
    "    def rangeSrc(title,segment=None):\n",
    "        if segment == None:\n",
    "            return df_headers.filter(F.lower(F.col('counter'))==title).select('srcline').collect()[0]['srcline']\n",
    "        else:\n",
    "            return df_headers.filter((F.lower(F.col('counter'))==title) & (F.col('segment')==segment)).select('srcline').collect()[0]['srcline']\n",
    "\n",
    "    # user-defined function for creating nested dictionary\n",
    "    def dotaDict(type,scrape_list):\n",
    "        text = []\n",
    "        srcline = []\n",
    "        types = []\n",
    "        for h in scrape_list:\n",
    "            types.append(type)\n",
    "            text.append(h.get_text())\n",
    "            srcline.append(h.sourceline)\n",
    "        arr = zip(types,text,srcline)\n",
    "        return arr\n",
    "\n",
    "    # created nested dictionary\n",
    "    ## header\n",
    "    headers = dotaDict('header',header_list)\n",
    "    ## heroes\n",
    "    heroes = dotaDict('hero',hero_name_list)\n",
    "    ## items\n",
    "    items = dotaDict('items/skills',item_list)\n",
    "\n",
    "    df_heroes = spark.createDataFrame(data=heroes,schema=schema)\n",
    "    df_items = spark.createDataFrame(data=items,schema=schema)\n",
    "    df_headers = spark.createDataFrame(data=headers,schema=schema)\n",
    "\n",
    "    ## Create another column for segment\n",
    "    # create threshold for segment\n",
    "    bad_against = rangeSrc('bad against...')\n",
    "    good_against = rangeSrc('good against...')\n",
    "    works_well = rangeSrc('works well with...')\n",
    "\n",
    "    # function for determining if the hero or item is a counter or not\n",
    "    def segment(srcline):\n",
    "        if srcline >= bad_against and srcline < good_against:\n",
    "            return 'Bad against...'\n",
    "        elif srcline >= good_against and srcline < works_well:\n",
    "            return 'Good against...'\n",
    "        elif srcline >= works_well:\n",
    "            return 'Works well with...'\n",
    "        else:\n",
    "            pass\n",
    "    segmentUDF = F.udf(lambda z: segment(z), StringType())\n",
    "\n",
    "    # add segment\n",
    "    df_heroes = df_heroes.withColumn('segment', segmentUDF('srcline'))\n",
    "    df_items = df_items.withColumn('segment', segmentUDF('srcline'))\n",
    "    df_headers = df_headers.withColumn('segment', segmentUDF('srcline'))\n",
    "\n",
    "    # ## Categorize items from skills in Others\n",
    "    # # create threshold for others\n",
    "    # bad_others = rangeSrc('Others','Bad against...')\n",
    "    # bad_items = rangeSrc('Items','Bad against...')\n",
    "    # good_others = rangeSrc('Others','Good against...')\n",
    "    # good_items = rangeSrc('Items','Good against...')\n",
    "    # works_others = rangeSrc('Others','Works well with...')\n",
    "    # works_items = rangeSrc('Items','Works well with...')\n",
    "\n",
    "    # # function for determining if the hero or item is a counter or not\n",
    "    # def segment2(srcline):\n",
    "    #     if (srcline >= bad_others and srcline < bad_items) or (srcline >= good_others and srcline < good_items) or (srcline >= works_others and srcline < works_items):\n",
    "    #         return 'others'\n",
    "    #     elif (srcline >= bad_against and srcline < bad_others) or (srcline >= good_against and srcline < good_others) or (srcline >= works_well and srcline < works_others):\n",
    "    #         return 'hero_others'\n",
    "    #     elif (srcline >= bad_items and srcline < good_against) or (srcline >= good_items and srcline < works_well) or (srcline >= works_items):\n",
    "    #         return 'item'\n",
    "    # segment2UDF = F.udf(lambda z: segment2(z), StringType())\n",
    "\n",
    "    # correct type\n",
    "    # df_heroes2 = df_heroes\n",
    "    # df_items2 = df_items.withColumn('type', segment2UDF('srcline'))\n",
    "\n",
    "    # join 2 dfs\n",
    "    df_conso = df_heroes.unionByName(df_items)\n",
    "    df_conso = df_conso.withColumn('hero',F.lit(hero)).select('hero','segment','type','counter','srcline')\n",
    "\n",
    "    df = df.unionByName(df_conso)\n",
    "    print('Scraping done: '+hero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hero: string (nullable = true)\n",
      " |-- segment: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- counter: string (nullable = true)\n",
      " |-- srcline: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31358.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:558)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/maedora/Repositories/dota2wiki-counter-crawler/dev_crawler.ipynb Cell 6\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/maedora/Repositories/dota2wiki-counter-crawler/dev_crawler.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/maedora/Repositories/dota2wiki-counter-crawler/dev_crawler.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m.\u001b[39;49mpartitionBy(\u001b[39m\"\u001b[39;49m\u001b[39mhero\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/maedora/Repositories/dota2wiki-counter-crawler/dev_crawler.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/maedora/Repositories/dota2wiki-counter-crawler/dev_crawler.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39ms3://pavluff-dev-web-scrapes-public/dota2counterCrawler/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31358.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:558)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"hero\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"s3://pavluff-dev-web-scrapes-public/dota2counterCrawler/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
